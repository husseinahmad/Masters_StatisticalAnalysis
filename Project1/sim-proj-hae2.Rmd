---
title: "Simulation Project"
author: "STAT 420, Summer 2018, Hussein Elmessilhy, hae2"
date: ''
output:
  html_document: 
    toc: yes
  pdf_document: default
  fig_caption: yes
urlcolor: cyan
---


## Simulation Study 1, Significance of Regression


**(a)** Introduction


In this study, we are trying to prove that the significance of regression test is a good indicator of whether a regression model is relevant or not to the data at hand.

To do so, data will be simulated from two different true models, one of them its output is function of all the predictors, while the other one its output is not a function of the predictors. 

To make sure of the variability of data to experiment with, each true model will be used with different values of error variance to generate the data. Also, many simulations will be generated to assess the consistency of our observations.

The two models output follow the below equations: 
\[
Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \epsilon_i.
\]

$\beta$ parameters have values in the significant model, namely (3,1,1,1) for the beta values respectively, while the non-significant model has $\beta_0$ equals to 3 while the others are 0.

Other parameters we are considering are sample size, which will be fixed to 25, and $\sigma$ values for the error will be in (1,5,10). Number of simulations to be used for each model-$\sigma$ combinations are 2500 to make sure of the consistency of our simulations.


**(b)** Methods

```{r}
birthday = 19850322
set.seed(birthday)
setwd(".\\")
study1_data <- read.csv("study_1.csv", header = TRUE, sep = ",")

generate_data = function(data, sigma, params){

  y <- (as.matrix(cbind(rep(1, nrow(data)), data[,-1])) %*% as.matrix(params))
  y <- y + rnorm(nrow(data) , 0, sigma)
  return (cbind(y, data[,-1]))

}

simulate_model = function(data, simulations, sigma, params){

  fStats <- rep(0, simulations)
  pVal <- rep(0, simulations)
  rSquared <- rep(0, simulations)
  betas <- matrix(0, nrow = simulations, ncol = length(params))
  se <- rep(0, simulations)
  
  for (i in 1:simulations)
  {
    data <- generate_data(data, sigma, params)
    model <- lm(y ~ ., data)
    model_summary <- summary(model)
    betas[i,] <- coef(model)
    rSquared[i] <- model_summary$r.squared
    fStats[i] <- model_summary$fstatistic[1]
    pVal[i] <- 1 - pf(model_summary$fstatistic[1], model_summary$fstatistic[2], model_summary$fstatistic[3])
    se[i] <- model_summary$sigma
  }
  
  result <- list()
  result$fStats <- fStats
  result$pVal <- pVal
  result$rSquared <- rSquared
  result$betas <- betas
  result$se <- se
  
  return (result)
}

study1_significant_1 <- simulate_model(study1_data, 2500, 1, c(3,1,1,1))
study1_significant_5 <- simulate_model(study1_data, 2500, 5, c(3,1,1,1))
study1_significant_10 <- simulate_model(study1_data, 2500, 10, c(3,1,1,1))

study1_nonSignificant_1 <- simulate_model(study1_data, 2500, 1, c(3,0,0,0))
study1_nonSignificant_5 <- simulate_model(study1_data, 2500, 5, c(3,0,0,0))
study1_nonSignificant_10 <- simulate_model(study1_data, 2500, 10, c(3,0,0,0))

```


**(c)** Results

```{r figs1_1, fig.height=8, fig.width=10,fig.cap="\\label{fig:figs1_1}Figure 1.1: F statistic for Signigicant and non-significant simulations, as well as the true expected F distribution if the model is non-significant"}

# Plotting charts for the f statistic

# charts when the model is not significant
par(mfrow=c(2,3))

hist(study1_nonSignificant_1$fStats, prob = TRUE, breaks = 25, 
     xlab = "F statistic", main = bquote("Non-significant model when" ~ sigma == .(1)), border = "dodgerblue", ylim = c(0,0.8))
x <- seq(0, 100, length = 25)    
curve(df(x, df1 = 3, df2 = nrow(study1_data) - 4  ),
 col = "darkorange", add = TRUE, lwd = 3)


hist(study1_nonSignificant_5$fStats, prob = TRUE, breaks = 25, 
     xlab = "F statistic", main = bquote("Non-significant model when" ~ sigma == .(5)), border = "dodgerblue" , ylim = c(0,0.8))
x <- seq(0, 10, length = 25)    
curve(df(x, df1 = 3, df2 = nrow(study1_data) - 4  ),
 col = "darkorange", add = TRUE, lwd = 3)

hist(study1_nonSignificant_10$fStats, prob = TRUE, breaks = 25, 
     xlab = "F statistic", main = bquote("Non-significant model when" ~ sigma == .(10)), border = "dodgerblue" , ylim = c(0,0.8))
x <- seq(0, 10, length = 25)    
curve(df(x, df1 = 3, df2 = nrow(study1_data) - 4  ),
 col = "darkorange", add = TRUE, lwd = 3)


#charts for f statistic when the model is significant

hist(study1_significant_1$fStats, prob = TRUE, breaks = 25, 
     xlab = "F statistic", main = bquote("Significant model when" ~ sigma == .(1)), border = "dodgerblue")
x <- seq(0, 10, length = 25)    
curve(df(x, df1 = 3, df2 = nrow(study1_data) - 4  ),
 col = "darkorange", add = TRUE, lwd = 3)


hist(study1_significant_5$fStats, prob = TRUE, breaks = 25, 
     xlab = "F statistic", main = bquote("Significant model when" ~ sigma == .(5)), border = "dodgerblue")
x <- seq(0, 10, length = 25)    
curve(df(x, df1 = 3, df2 = nrow(study1_data) - 4  ),
 col = "darkorange", add = TRUE, lwd = 3)

hist(study1_significant_10$fStats, prob = TRUE, breaks = 25, 
     xlab = "F statistic", main = bquote("Significant model when" ~ sigma == .(10)), border = "dodgerblue")
x <- seq(0, 10, length = 25)    
curve(df(x, df1 = 3, df2 = nrow(study1_data) - 4  ),
 col = "darkorange", add = TRUE, lwd = 3)


```



```{r figs1_2, fig.height=8, fig.width=10,fig.cap="\\label{fig:figs1_2}Figure 1.2: R-Squared for Signigicant and non-significant simulations"}

# Plotting charts for the R-Squared

# charts when the model is not significant
par(mfrow=c(2,3))

hist(study1_nonSignificant_1$rSquared, prob = TRUE, breaks = 25, 
     xlab = "R-Squared", main = bquote("Non-significant model when" ~ sigma == .(1)), border = "dodgerblue")



hist(study1_nonSignificant_5$rSquared, prob = TRUE, breaks = 25, 
     xlab = "R-Squared", main = bquote("Non-significant model when" ~ sigma == .(5)), border = "dodgerblue")


hist(study1_nonSignificant_10$rSquared, prob = TRUE, breaks = 25, 
     xlab = "R-Squared", main = bquote("Non-significant model when" ~ sigma == .(10)), border = "dodgerblue")


#charts when the model is significant

hist(study1_significant_1$rSquared, prob = TRUE, breaks = 25, 
     xlab = "R-Squared", main = bquote("Significant model when" ~ sigma == .(1)), border = "dodgerblue")   


hist(study1_significant_5$rSquared, prob = TRUE, breaks = 25, 
     xlab = "R-Squared", main = bquote("Significant model when" ~ sigma == .(5)), border = "dodgerblue")   

hist(study1_significant_10$rSquared, prob = TRUE, breaks = 25, 
     xlab = "R-Squared", main = bquote("Significant model when" ~ sigma == .(10)), border = "dodgerblue") 

```

```{r figs1_3, fig.height=8, fig.width=10,fig.cap="\\label{fig:figs1_3}Figure 1.3: P value for Signigicant and non-significant simulations"}

# Plotting charts for the R-Squared

# charts when the model is not significant
par(mfrow=c(2,3))

hist(study1_nonSignificant_1$pVal, prob = TRUE, breaks = 25, 
     xlab = "P value", main = bquote("Non-significant model when" ~ sigma == .(1)), border = "dodgerblue")
   

hist(study1_nonSignificant_5$pVal, prob = TRUE, breaks = 25, 
     xlab = "P value", main = bquote("Non-significant model when" ~ sigma == .(5)), border = "dodgerblue")


hist(study1_nonSignificant_10$pVal, prob = TRUE, breaks = 25, 
     xlab = "P value", main = bquote("Non-significant model when" ~ sigma == .(10)), border = "dodgerblue")


#charts when the model is significant

hist(study1_significant_1$pVal, prob = TRUE, breaks = 25, 
     xlab = "P value", main = bquote("Significant model when" ~ sigma == .(1)), border = "dodgerblue")   


hist(study1_significant_5$pVal, prob = TRUE, breaks = 25, 
     xlab = "P value", main = bquote("Significant model when" ~ sigma == .(5)), border = "dodgerblue")


hist(study1_significant_10$pVal, prob = TRUE, breaks = 25, 
     xlab = "P value", main = bquote("Significant model when" ~ sigma == .(10)), border = "dodgerblue")

```

**(D)** Discussion

The regression models significance tests were able to conclude a right decision (model is significant by rejecting the null hypotheses) in each of the significant models simulations per $\sigma$ equal to the following (based on $\alpha$ value equal to 0.05):

- When $\sigma$ = 1 : `r  100 * (sum(study1_significant_1$pVal < 0.05) / 2500)`%

- When $\sigma$ = 5 : `r   100 * (sum(study1_significant_5$pVal < 0.05) / 2500)`%

- When $\sigma$ = 10 : `r  100 * (sum(study1_significant_10$pVal < 0.05) / 2500)`%

You can notice that we are not doing a good job when the sigma gets bigger as the model tends to be non-significant with more and more null hypotheses failed to be rejected.

On the other hand, the regression models in the non-significant models were able to conclude a right decision (fail to reject the null hypotheses) in each of the non-significant models simulations per sigma equal to the following (based on $\alpha$ value equal to 0.05):

- When $\sigma$ = 1 : `r  100 * (sum(study1_nonSignificant_1$pVal > 0.05) / 2500)`%

- When $\sigma$ = 5 : `r  100 * (sum(study1_nonSignificant_5$pVal > 0.05) / 2500)`%

- When $\sigma$ = 10 : `r  100 * (sum(study1_nonSignificant_10$pVal > 0.05) / 2500)`%



Non-significant model true distribution for F statistic are expected be an F distribution with df1 = 3 and df2 = 21, if you check **Figure 1.1** , you will notice that simulations for the non-significant true model (shown in blue) are following the same F distribution expected by the true model (shown in Orange). This tells us that our simulations are working as expected for the non-significant models regardless of the $\sigma$ value. 

Significant models true distribution on the other hand are not supposed to follow an F distribution, we can see that this is true for the case where $\sigma$ is equal to 1, this however starts to change when $\sigma$ gets bigger as the simulations tends to get closer to an F distribution, another indicator that the regression models are not working well when $\sigma$ is getting higher.


**Figure 1.2** shows R-Squared metric when the model is significant and non-significant for the different $\sigma$ values. As expected, regression for non-significant models should have low R-Squared values as we are not explaining the variance of the output, which is true. Same happened, low R-Squared values, with significant models with high $\sigma$ values as noise became unexplained by our regression models among most of the simulations. 

We can though see good results for the regression of significant model when $\sigma$ is 1 with R-Squared mean equal to `r mean(study1_significant_1$rSquared)`, a high value that indicates we interpret the variance in the data pretty well.

Finally, as expected, and in accordance with the previous results, p-value in **Figure 1.3** for the significant model simulations is very low when $\sigma$ was equal to 1, which means null hypotheses was consistently rejected. This started to change when $\sigma$ gets bigger as we weren't able to reject the same amount of null hypotheses. Non-significant models p-value are kind of uniformly distributed, which means a lot of different values were picked, and null hypotheses was frequently accepted as desired.

**Conclusion**

    * Significantce of regression test was proven to be successful with non-significant models (fail to reject null hyotheses most of the time)
    * Significantce of regression test was proven to be successful with significant models (reject null hyotheses most of the time) when $\sigma$ = 1, and wasn't that successful when the noise got higher
    * F statistic follows an F distribution for the non-significant models, and seemed to follow it as well in the significant model when $\sigma$ gets higher





## Simulation Study 2, Using RMSE for Selection?


**(a)** Introduction

In this study, we are investigating how good the RMSE metric for evaluating a model. To do so, we will assume a true linear model that we know its representation and its output follows the formula :
\[
Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \beta_4 x_{i4} + \beta_5 x_{i5} + \beta_6 x_{i6} + \epsilon_i.
\]

Our data though has 9 variables, so 3 of them has no effect on the output and thus a good model will not include them in a linear regression model as predictors.

The goal of this study is to check if RMSE metric over testing data is able to select the model with the relevant 6 predictors as the best model. 

Although there are 2 ^ 9 models to consider (including null model and the expected model) based on the combinations of the variables, but we will only consider the following 9 out of them (incrementally nested):


    + Model1 : x1 predictor --> 1 predictor
    + Model2 : x1, x2 predictors --> 2 predictor
    + Model3 : x1, x2, x3 predictorsr --> 3 predictor
    + Model4 : x1, x2, x3, x4 predictors --> 4 predictor
    + Model5 : x1, x2, x3, x4, x5 predictors --> 5 predictor
    + Model6 : x1, x2, x3, x4, x5, x6 predictors --> 6 predictor
    + Model7 : x1, x2, x3, x4, x5, x6, x7 predictors --> 7 predictor
    + Model8 : x1, x2, x3, x4, x5, x6, x7, x8 predictors --> 8 predictor
    + Model9 : x1, x2, x3, x4, x5, x6, x7, x8, x9 predictors --> 9 predictor

We will build simulations of our true model for the output and build linear regression models based on the mentioned 9 models and check whether our metric, RMSE test, will be able to pick the best model (the one with six predictors) most of the time. 

Different $\sigma$ values will be considered to assess the consistency of our metric.

**(b)** Methods

```{r}
birthday = 19850322
set.seed(birthday)
setwd(".\\")
study2_data <- read.csv("study_2.csv", header = TRUE, sep = ",")

generate_data = function(data, sigma, params){

  y <- (as.matrix(cbind(rep(1, nrow(data)), data[,-1])) %*% as.matrix(params))
  y <- y + rnorm(nrow(data) , 0, sigma)
  return (cbind(y, data[,-1]))

}

simulate_model = function(data, simulations, sigma, params, predictors_number, test_count){

  best_model_train <- rep(0, simulations)
  best_model_test <- rep(0, simulations)
  rSquared_total <- rep(0, predictors_number)
  rmse_train_total <- rep(0, predictors_number)
  rmse_test_total <- rep(0, predictors_number)
  
  for (simulation_counter in 1:simulations)
  {
    data <- generate_data(data, sigma, params)
    tst_idx = sample(1:nrow(data), test_count)
    train_data <- data[-tst_idx,]
    test_data <- data[tst_idx,]
    
    current_best_train_rmse <- Inf
    current_best_test_rmse <- Inf
    current_best_train_rmse_index <- 0
    current_best_test_rmse_index <- 0
    
    for(predictors_counter in 1:predictors_number)
    {
      # create a dynamic formula based on the number of predictors
      xnam <- paste0("x", 1:predictors_counter)
      fmla <- as.formula(paste("y ~ ", paste(xnam, collapse= "+")))
      model <- lm(fmla, train_data)
      model_summary <- summary(model)
      rSquared_total[predictors_counter] <- rSquared_total[predictors_counter] + model_summary$r.squared
      
      # updating total RMSE train and test
      rmse_train <- sqrt(sum((train_data$y - model$fitted.values) ^ 2)/nrow(train_data))
      rmse_train_total[predictors_counter] <- rmse_train_total[predictors_counter] + rmse_train
      
      fitted_test <- predict(model, test_data[,-1])
      rmse_test <- sqrt(sum((test_data[,1] - fitted_test) ^ 2)/nrow(test_data))
       rmse_test_total[predictors_counter] <- rmse_test_total[predictors_counter] + rmse_test
       
       # update current best model based on train and test RMSE for this simulation
       if(rmse_train < current_best_train_rmse)
       {
         current_best_train_rmse <- rmse_train
         current_best_train_rmse_index <- predictors_counter
       }
       
       if(rmse_test < current_best_test_rmse)
       {
         current_best_test_rmse <- rmse_test
         current_best_test_rmse_index <- predictors_counter
       }
    }
    
    #update index of best model for this simulation
    best_model_train[simulation_counter] <- current_best_train_rmse_index
    best_model_test[simulation_counter] <- current_best_test_rmse_index
    
  }
  
  result <- list()
  result$best_model_train <- best_model_train
  result$best_model_test <- best_model_test
  result$rSquared_mean <- rSquared_total / simulations
  result$rmse_train_mean <- rmse_train_total / simulations
  result$rmse_test_mean <- rmse_test_total / simulations
  
  return (result)
}



study2_sigma1_models <- simulate_model(data = study2_data, simulations = 1000, sigma = 1, params = c(0, 5, -4, 1.6, -1.1, 0.7, 0.3, 0, 0, 0),  predictors_number = 9, test_count = 250)

study2_sigma2_models <- simulate_model(data = study2_data, simulations = 1000, sigma = 2, params = c(0, 5, -4, 1.6, -1.1, 0.7, 0.3, 0, 0, 0),  predictors_number = 9, test_count = 250)

study2_sigma4_models <- simulate_model(data = study2_data, simulations = 1000, sigma = 4, params = c(0, 5, -4, 1.6, -1.1, 0.7, 0.3, 0, 0, 0),  predictors_number = 9, test_count = 250)


```



**(c)** Results

```{r figs2_1, fig.height=10, fig.cap="\\label{fig:figs2_1}Figure 2.1: RMSE train and RMSE test as a function of number of predictors"}

###################### sigma is 1 ###################

par(mfrow=c(3,2))

plot(study2_sigma1_models$rmse_train_mean, xlab = 'Model/Number of predictors', ylab = 'RMSE Train', lwd = 2, col = 'darkorange', main = 'Plot when Sigma is 1')

lines(1:9, study2_sigma1_models$rmse_train_mean, lwd = 3, lty = 1, col = "darkorange")

plot(study2_sigma1_models$rmse_test_mean, xlab = 'Model/Number of predictors', ylab = 'RMSE Test', lwd = 2, col = 'darkorange', main = 'Plot when Sigma is 1')

lines(1:9, study2_sigma1_models$rmse_test_mean, lwd = 3, lty = 1, col = "darkorange")

###################### sigma is 2 ###################

#par(mfrow=c(1,2))
plot(study2_sigma2_models$rmse_train_mean, xlab = 'Model/Number of predictors', ylab = 'RMSE Train', lwd = 2, col = 'orange', main = 'Plot when Sigma is 2')

lines(1:9, study2_sigma2_models$rmse_train_mean, lwd = 3, lty = 1, col = "darkorange")

plot(study2_sigma2_models$rmse_test_mean, xlab = 'Model/Number of predictors', ylab = 'RMSE Test', lwd = 2, col = 'orange', main = 'Plot when Sigma is 2')

lines(1:9, study2_sigma2_models$rmse_test_mean, lwd = 3, lty = 1, col = "darkorange")

###################### sigma is 4 ###################

#par(mfrow=c(1,2))
plot(study2_sigma4_models$rmse_train_mean, xlab = 'Model/Number of predictors', ylab = 'RMSE Train', lwd = 2, col = 'orange', main = 'Plot when Sigma is 4')

lines(1:9, study2_sigma4_models$rmse_train_mean, lwd = 3, lty = 1, col = "darkorange")

plot(study2_sigma4_models$rmse_test_mean, xlab = 'Model/Number of predictors', ylab = 'RMSE Test', lwd = 2, col = 'orange', main = 'Plot when Sigma is 4')

lines(1:9, study2_sigma4_models$rmse_test_mean, lwd = 3, lty = 1, col = "darkorange")
```

```{r figs2_2, fig.cap="\\label{fig:figs2_2}Figure 2.2: Count of picked models based on best RMSE test"}
par(mfrow=c(1,3))

hist(study2_sigma1_models$best_model_test, xlab = 'Model Index', ylab = 'Picked model based on best RMSE test', lwd = 2, col = 'orange', main = 'Plot when Sigma is 1')


hist(study2_sigma2_models$best_model_test, xlab = 'Model Index', ylab = 'Picked model based on best RMSE test', lwd = 2, col = 'orange', main = 'Plot when Sigma is 2')


hist(study2_sigma4_models$best_model_test, xlab = 'Model Index', ylab = 'Picked model based on best RMSE test', lwd = 2, col = 'orange', main = 'Plot when Sigma is 4')


```



**(D)** Discussion

Based on the analysis performed, RMSE train alway improves (decreases) when more predictors are used, we can see this in **Figure 2.1** which shows the average RMSE train as a function of the number of predictors. This property didn't help us in identifying the expected best model (the one with the first 6 predictors only).

As a proof for this, out of the 1000 simulations, the model with the 9 predictors was picked `r sum(study2_sigma1_models$best_model_train == 9)` when $\sigma$ was 1, and `r sum(study2_sigma2_models$best_model_train == 9)` when $\sigma$ was 2 and `r sum(study2_sigma4_models$best_model_train == 9)` when $\sigma$ was 4 if we are to use RMSE train for selection.

On the other hand, RMSE test has more interesting results, in case you were wondering if the curve is not always decreasing in **Figure 2.1** I assure you it's not, average RMSE test when number of predictors is 6 and $\sigma$ equals 1 is `r study2_sigma1_models$rmse_test_mean[6]`, while it's `r study2_sigma1_models$rmse_test_mean[9]` when number of predictors is 9, a bit higher and thus worse RMSE metric value. 

**Figure 2.2** confirms these findings, model with 6 predictors was successfully picked as the best model most of the simulations when $\sigma$ is 1 and 2, however, when $\sigma$ became 4, noise affected the regression model and selection on RMSE test wasn't as stable metric to pick the best model as it was before.

**Conclusion**

    * RMSE train is not sufficient to pick best models as it always decreases when there are more predictors regardless they are relevant or not
    * RMSE test is a good metric to be used to pick the best regression model, it's able to pick a model with less number of predictors when it's more relevant to the problem at hand
    * Noise can affect RMSE test as the data becomes more stochastic


## Simulation Study 3, Power


**(a)** Introduction

In this study, we are investigating the impact of different factors on the power of the significance of regression test, these factors are sample size, $\beta$ values (how big its absolute value), and noise level $\sigma$.

In order to do so, we will run simulations with different values of these factors and see how the power of the significance test gets affected as a function of the different factors. 

The model we are building has only 1 predictor with values in the range [-2,2] with a step size equal to 0.1. Note that model when $\beta$ equals to 0 is not taken into consideration as it doesn't affect the power equation known as :

Power = P[Reject H0 | H1 True]

Surely, H1, the alternative hypotheses, is not true when $\beta$ value is 0.

As for the $\sigma$ values, we will simulate data with values equal to 1, 2, and 4, while sample size to experiment with will take the values 10, 20, and 30.

**(b)** Methods

```{r}

birthday = 19850322
set.seed(birthday)

simulate_model = function(sigma, sample_sizes, simulations, beta_values)
{
  power_result <- matrix(0, nrow = length(sample_sizes), ncol = length(beta_values))
  row.names(power_result) <- sample_sizes
  colnames(power_result) <- beta_values
  for(n in sample_sizes)
  {
    x_values = seq(0, 5, length = n)
    
   
    for( beta1 in beta_values )
    {
      # same y value for all simulations without noise, different noise will be added in each record for each simulation
      base_y <- (x_values * beta1)
      # after this, each data set/simulation will be represented in a row
      simulation_vectors <- t(replicate(simulations, base_y))
      # add noise to each element, it's one sigma anyway
      noise_matrix <- rnorm(n * simulations , 0, sigma)
      noise_matrix <- matrix(noise_matrix, nrow = simulations, ncol = n)
      simulation_vectors <- simulation_vectors + noise_matrix
      simulations_p_values <- apply(simulation_vectors, 1, function(x) summary(lm(x ~ x_values))$coefficients[2,4])
      
      power_result[as.character(n), as.character(beta1)] <- (sum(simulations_p_values < 0.05 )) / simulations
    }
  }
  
  return(power_result)
}

 # we have to ignore beta1 when it equals zero, as null hypotheses is true in this case and thus doesn't affect power formula
beta_values <- setdiff(seq(-2,2, by = 0.1), c(0))
print("Processing sigma 1 data")
study3_sigma1_1kSims <- simulate_model(1, c(10,20,30), 1000 ,beta_values)
print("Processing sigma 2 data")
study3_sigma2_1kSims <- simulate_model(2, c(10,20,30), 1000 ,beta_values)
print("Processing sigma 4 data")
study3_sigma4_1kSims <- simulate_model(4, c(10,20,30), 1000 ,beta_values)

print("Processing different simulations numbers data")
study3_sigma1_2kSims <- simulate_model(1, c(10), 2000 ,beta_values)
study3_sigma1_100Sims <- simulate_model(1, c(10), 100 ,beta_values)
```


**(c)** Results

```{r figs3_1, fig.height=5, fig.width=10, fig.cap="\\label{fig:figs3_1}Figure 3.1: Power curve as a function of sigma, sample size, and beta values"}

########################## Drawing for sigma = 1 #############################
par(mfrow=c(1,3))

plot(study3_sigma1_1kSims['10',] ~ colnames(study3_sigma1_1kSims), xlab = "Beta values",
     ylab = "Power",
     main = bquote("Power curve when " ~ sigma == .(1)),
     pch  = 20,
     cex  = 2,
     col  = "grey", type="n")

lines(colnames(study3_sigma1_1kSims), study3_sigma1_1kSims['10',], lwd = 3, lty = 1, col = "darkorange")

lines(colnames(study3_sigma1_1kSims), study3_sigma1_1kSims['20',], lwd = 3, lty = 1, col = "darkblue")

lines(colnames(study3_sigma1_1kSims), study3_sigma1_1kSims['30',], lwd = 3, lty = 1, col = "darkgreen")

#lines(colnames(study3_sigma1_5kSims), study3_sigma1_5kSims['30',], lwd = 3, lty = 1, col = "red")

legend("bottomright", c("n=10", "n=20", "n=30"),  lwd = 2,
       col = c("darkorange", "darkblue", "darkgreen"))

########################## Drawing for sigma = 2 #############################
plot(study3_sigma2_1kSims['10',] ~ colnames(study3_sigma2_1kSims), xlab = "Beta values",
     ylab = "Power",
     main = bquote("Power curve when " ~ sigma == .(2)),
     pch  = 20,
     cex  = 2,
     col  = "grey", type="n")

lines(colnames(study3_sigma2_1kSims), study3_sigma2_1kSims['10',], lwd = 3, lty = 1, col = "darkorange")

lines(colnames(study3_sigma2_1kSims), study3_sigma2_1kSims['20',], lwd = 3, lty = 1, col = "darkblue")

lines(colnames(study3_sigma2_1kSims), study3_sigma2_1kSims['30',], lwd = 3, lty = 1, col = "darkgreen")

legend("bottomright", c("n=10", "n=20", "n=30"),  lwd = 2,
       col = c("darkorange", "darkblue", "darkgreen"))

########################## Drawing for sigma = 4 #############################
plot(study3_sigma4_1kSims['10',] ~ colnames(study3_sigma4_1kSims), xlab = "Beta values",
     ylab = "Power",
     main = bquote("Power curve when " ~ sigma == .(4)),
     pch  = 20,
     cex  = 2,
     col  = "grey", type="n")

lines(colnames(study3_sigma4_1kSims), study3_sigma4_1kSims['10',], lwd = 3, lty = 1, col = "darkorange")

lines(colnames(study3_sigma4_1kSims), study3_sigma4_1kSims['20',], lwd = 3, lty = 1, col = "darkblue")

lines(colnames(study3_sigma4_1kSims), study3_sigma4_1kSims['30',], lwd = 3, lty = 1, col = "darkgreen")

legend("bottomright", c("n=10", "n=20", "n=30"),  lwd = 2,
       col = c("darkorange", "darkblue", "darkgreen"))

```

```{r figs3_2, fig.height=5, fig.width=10, fig.cap="\\label{fig:figs3_2}Figure 3.2: Avergae Power as a function of sigma, sample size, and beta values separately"}

par(mfrow=c(1,3))
study3_sigma_avgs <- c(mean(study3_sigma1_1kSims), mean(study3_sigma2_1kSims), mean(study3_sigma4_1kSims))
barplot(study3_sigma_avgs, names.arg = c("sigma=1", "sigma=2", "sigma=4"), ylab = "Power average", xlab = expression(sigma), main = 'Average power against sigma', col = "darkgreen")

study3_all <- rbind(study3_sigma1_1kSims, study3_sigma2_1kSims, study3_sigma4_1kSims)
study3_beta_avgs <- apply(study3_all, c(2), mean)
barplot(study3_beta_avgs, ylab = "Power average", xlab = expression(beta), main = 'Average power against beta', col = "darkorange")

study3_all_n10 <- rbind(study3_sigma1_1kSims['10',], study3_sigma2_1kSims['10',], study3_sigma4_1kSims['10',])

study3_all_n20 <- rbind(study3_sigma1_1kSims['20',], study3_sigma2_1kSims['20',], study3_sigma4_1kSims['20',])

study3_all_n30 <- rbind(study3_sigma1_1kSims['30',], study3_sigma2_1kSims['30',], study3_sigma4_1kSims['30',])

study3_n_avgs <- c(mean(study3_all_n10), mean(study3_all_n20), mean(study3_all_n30))
barplot(study3_n_avgs, names.arg = c("n=10", "n=20", "n=30"), ylab = "Power average", xlab = 'Sample size', main = 'Average power against sample size' , col = "darkblue")
```

```{r figs3_3, fig.height=5, fig.cap="\\label{fig:figs3_3}Figure 3.3: Power curve as a function of number of simulations"}

plot(study3_sigma1_1kSims['10',] ~ colnames(study3_sigma1_1kSims), xlab = "Beta values",
     ylab = "Power",
     main = bquote("Power curve when sample size is 10 and " ~ sigma == .(1)),
     pch  = 20,
     cex  = 2,
     col  = "grey", type="n")

lines(colnames(study3_sigma1_1kSims), study3_sigma1_1kSims['10',], lwd = 3, lty = 1, col = "darkorange")

lines(colnames(study3_sigma1_2kSims), study3_sigma1_2kSims['10',], lwd = 3, lty = 1, col = "darkblue")

lines(colnames(study3_sigma1_100Sims), study3_sigma1_100Sims['10',], lwd = 3, lty = 1, col = "red")

legend("bottomright", c("Simulations=1000", "Simulations=2000", "Simulations=100"),  lwd = 2,
       col = c("darkorange", "darkblue", "red"))
```
**(D)** Discussion
 
**Figure 3.1** shows the impact of the different factors on the power curve, regardless of $\sigma$ value, power gets lower when the absolute value of $\beta$ decreases, in other words, when it gets closer to 0. This can be shown in the 3 curves. 

Also, sample size has an impact on the power, the bigger the sample size, the better its power curve. Same figure shows that curves with bigger sample size stays on 1 value with more $\beta$ values and in worst case don't go as low as their corresponding curves (curves with same $\sigma$ and $\beta$).

As for $\sigma$ impact, it goes to the other direction, a higher $\sigma$ leads to more noise and thus to less confident models with worse power curve. For example, Curve on the right on **Figure 3.1**, which has $\sigma$ equals to 4, has a maximum power value less than 0.7 unlike the curve on the left which has $\sigma$ equals 1. It also goes faster to lower values of power.

As a confirmation for these findings, **Figure 3.2** shows the impact of the different factors separately on the average of the power. From the left, bigger $\sigma$ causes lower power regardless of the other factors, and then in the second bar chart, as $\beta$ value approaches 0 power decreases, and finally, the right chart shows the sample size impact, which causes better average power when it increases.

As a final investigation on the other factors, or let's call it signals, that may affect the power. Intuitevly, $\alpha$ should be a factor as it's the value at which we decide whether a test is rejected or not and thus will definitely affect the power value as the equation in the introduction shows, but is the number of simulations another factor that may affect power ? 

**Figure 3.3** answers this question by showing that 1000 simulations was enough to get us sufficient and stable results, that's because pushing the number of simulations to 2000 didn't affect the power curve by any means when the $\sigma$ was 1 and sample size was 10. However, if the number of simulations was 100 as shown in the chart in red, we wouldn't have gotten that stable and smooth results.

**Conclusion**

    * Bigger $\sigma$ has a negative impact on power
    * As $\beta$ get closes to 0, power gets lower
    * Bigger sample size has a positive impact on power
    * 1000 simulations are sufficient to get consistent and reasonable results
    